{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyMNUawcX4rZoTCxtFNP03T3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"l2sqNeAlUZhn","executionInfo":{"status":"ok","timestamp":1732283903648,"user_tz":-660,"elapsed":2325,"user":{"displayName":"Jamil","userId":"02624495780327096902"}}},"outputs":[],"source":["import pandas as pd\n","import re\n","import os\n","from scipy.stats import entropy"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"2ZKX6dLxUe6t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284919826,"user_tz":-660,"elapsed":5772,"user":{"displayName":"Jamil","userId":"02624495780327096902"}},"outputId":"d5caa145-7691-4eb3-ea2f-e262978d45b8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","def read_and_combine_csv_files(directory_path):\n","    \"\"\"\n","    Reads CSV files from the specified directory. If there's only one CSV file, it reads that file directly.\n","    If there are multiple CSV files, it combines them into a single DataFrame.\n","\n","    Args:\n","        directory_path (str): Path to the directory containing the CSV file(s).\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame with the combined data from all CSV files in the directory, including additional\n","                      process name columns if applicable.\n","    \"\"\"\n","    # List all CSV files in the directory\n","    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n","\n","    # Check if there are any CSV files in the directory\n","    if not csv_files:\n","        raise ValueError(\"No CSV files found in the specified directory.\")\n","\n","    # If there's only one CSV file, read it directly\n","    if len(csv_files) == 1:\n","        df = pd.read_csv(os.path.join(directory_path, csv_files[0]))\n","    else:\n","        # If there are multiple CSV files, read and combine them\n","        df = pd.concat((pd.read_csv(os.path.join(directory_path, f)) for f in csv_files), ignore_index=True)\n","\n","    # Add process name columns if the relevant columns are present\n","    if 'process.executable' in df.columns:\n","        df['process.name'] = df['process.executable'].apply(lambda x: x.split('\\\\')[-1])\n","    if 'process.parent.executable' in df.columns:\n","        df['process.parent.name'] = df['process.parent.executable'].apply(lambda x: x.split('\\\\')[-1])\n","\n","    return df"],"metadata":{"id":"uZ1R-_npUraV","executionInfo":{"status":"ok","timestamp":1732284925806,"user_tz":-660,"elapsed":3,"user":{"displayName":"Jamil","userId":"02624495780327096902"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def read_csvs_from_subfolders(root_folder):\n","    \"\"\"\n","    Reads CSV files from each subfolder in the specified root folder. For each subfolder, it reads CSV files\n","    using the `read_and_combine_csv_files` function and returns a list of DataFrames.\n","\n","    Args:\n","        root_folder (str): Path to the root folder containing subfolders with CSV files.\n","\n","    Returns:\n","        list: A list of DataFrames, each corresponding to the combined data from the CSV files in a subfolder.\n","    \"\"\"\n","    dataframes = []\n","\n","    # Iterate over each subfolder in the root folder\n","    for subfolder in os.listdir(root_folder):\n","        subfolder_path = os.path.join(root_folder, subfolder)\n","\n","        # Check if the path is a directory\n","        if os.path.isdir(subfolder_path):\n","            try:\n","                # Use the previously defined function to read/merge CSV files in the subfolder\n","                df = read_and_combine_csv_files(subfolder_path)\n","                dataframes.append(df)  # Add the DataFrame to the list\n","            except ValueError as e:\n","                print(f\"Warning: {e} in subfolder {subfolder_path}. Skipping this subfolder.\")\n","\n","    return dataframes\n"],"metadata":{"id":"KcLQpFBMUvRt","executionInfo":{"status":"ok","timestamp":1732284932090,"user_tz":-660,"elapsed":527,"user":{"displayName":"Jamil","userId":"02624495780327096902"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["root_folder = '/content/drive/MyDrive/Colab Notebooks/paper'\n","dataframes = read_csvs_from_subfolders(root_folder)\n"],"metadata":{"id":"TCew1Fz0UzHG","executionInfo":{"status":"ok","timestamp":1732284941650,"user_tz":-660,"elapsed":7041,"user":{"displayName":"Jamil","userId":"02624495780327096902"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from scipy.stats import entropy\n","\n","# Load the dataset and sample\n","def load_and_sample_data(df):\n","    return df.copy(deep=True)#.sample(frac=1.0)\n","\n","# Clean the data by removing columns with identical values or high missing values\n","def clean_data(df, threshold=0.9):\n","    same_value_columns = [col for col in df.columns if df[col].nunique() == 1]\n","    missing_value_columns = [col for col in df.columns if df[col].isna().mean() > threshold]\n","    columns_to_drop = set(same_value_columns + missing_value_columns)\n","    return df.drop(columns=columns_to_drop)\n","\n","# Engineer features from the 'event.action' column\n","def engineer_event_action_features(df, engineered_features):\n","    selected_actions = [\n","        'File created (rule: FileCreate)', 'File Delete archived (rule: FileDelete)',\n","        'File creation time changed (rule: FileCreateTime)', 'Registry value set (rule: RegistryEvent)',\n","        'Process Create (rule: ProcessCreate)', 'Pipe Created (rule: PipeEvent)'\n","    ]\n","\n","    df['event.action_filtered'] = df['event.action'].apply(lambda x: x if x in selected_actions else None)\n","    one_hot_encoded_actions = pd.get_dummies(df['event.action_filtered']).astype(int)\n","    short_column_names = {col: col.split(' (rule')[0].replace(' ', '_') for col in one_hot_encoded_actions.columns}\n","    engineered_features = pd.concat([engineered_features, one_hot_encoded_actions.rename(columns=short_column_names)], axis=1)\n","    return engineered_features\n","\n","# Engineer features from the 'event.type' column\n","def engineer_event_type_features(df, engineered_features):\n","    def categorize_event_type(event_type):\n","        categories = {\n","            'process-related': ['process'],\n","            'file-related': ['file'],\n","            'network-related': ['network'],\n","            'error-related': ['error'],\n","            'driver-related': ['driver'],\n","            'registry-related': ['configuration, registry'],\n","        }\n","        for category, types in categories.items():\n","            if event_type in types:\n","                return category\n","        return 'miscellaneous'\n","\n","    df['event_type_group'] = df['event.category'].apply(categorize_event_type)\n","    event_type_encoded = pd.get_dummies(df['event_type_group']).astype(int)\n","    engineered_features = pd.concat([engineered_features, event_type_encoded[['process-related', 'file-related', 'network-related', 'driver-related', 'registry-related']]], axis=1)\n","    return engineered_features\n","\n","# Engineer features from 'process.executable' column\n","def engineer_process_executable_features(df, engineered_features):\n","    suspicious_keywords = [\"Temp\", \"AppData\", \"Roaming\", \"Startup\", \"Downloads\", \"ProgramData\", \"Users\"]\n","    trusted_dirs = ['C:\\\\Windows\\\\System32', 'C:\\\\Program Files', 'C:\\\\Windows']\n","\n","    df['suspicious_path'] = df['process.executable'].apply(lambda x: any(keyword in x for keyword in suspicious_keywords) if isinstance(x, str) else 0)\n","    df['system_executable'] = df['process.executable'].apply(lambda x: any(x.startswith(trusted) for trusted in trusted_dirs) if isinstance(x, str) else 0)\n","    df['path_length'] = df['process.executable'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","    df['directory_depth'] = df['process.executable'].apply(lambda x: x.count('\\\\') if isinstance(x, str) else 0)\n","\n","    engineered_features['suspicious_path'] = df['suspicious_path'].astype(int)\n","    engineered_features['system_executable'] = df['system_executable'].astype(int)\n","    engineered_features['path_length'] = df['path_length']\n","    engineered_features['directory_depth'] = df['directory_depth']\n","\n","    return engineered_features\n","\n","# Engineer features from 'process.name' and 'process.parent.name'\n","def engineer_process_name_features(df, engineered_features):\n","    process_name_freq = df['process.name'].value_counts()\n","    parent_name_freq = df['process.parent.name'].value_counts()\n","\n","    df['process_name_freq'] = df['process.name'].map(process_name_freq)\n","    df['parent_name_freq'] = df['process.parent.name'].map(parent_name_freq)\n","    df['process_vs_parent_freq_ratio'] = df['process_name_freq'] / (df['parent_name_freq'] + 1)\n","    df['process_name_length'] = df['process.name'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","\n","    engineered_features['process_vs_parent_freq_ratio'] = df['process_vs_parent_freq_ratio']\n","    engineered_features['process_name_length'] = df['process_name_length']\n","\n","    return engineered_features\n","\n","# Engineer additional process and parent process features\n","def engineer_parent_process_features(df, engineered_features):\n","    system_directories = ['C:\\\\Windows\\\\System32', 'C:\\\\Program Files']\n","\n","    df['executable_depth'] = df['process.executable'].apply(lambda x: x.count('\\\\') if isinstance(x, str) else 0)\n","    df['parent_executable_depth'] = df['process.parent.executable'].apply(lambda x: x.count('\\\\') if isinstance(x, str) else 0)\n","    df['executable_depth_diff'] = abs(df['executable_depth'] - df['parent_executable_depth'])\n","\n","    df['parent_is_system_executable'] = df['process.parent.executable'].apply(\n","        lambda x: 1 if isinstance(x, str) and any(dir in x for dir in system_directories) else 0)\n","\n","    df['process_extension'] = df['process.executable'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) and '.' in x else None)\n","    df['parent_process_extension'] = df['process.parent.executable'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) and '.' in x else None)\n","    df['extension_similarity'] = df.apply(lambda row: 1 if row['process_extension'] == row['parent_process_extension'] else 0, axis=1)\n","\n","    engineered_features['executable_depth_diff'] = df['executable_depth_diff']\n","    engineered_features['parent_is_system_executable'] = df['parent_is_system_executable']\n","    engineered_features['extension_similarity'] = df['extension_similarity']\n","\n","    return engineered_features\n","\n","# Calculate entropy for the 'file.name' column\n","def calculate_entropy(string):\n","    if not string:\n","        return 0\n","    probabilities = [float(string.count(c)) / len(string) for c in set(string)]\n","    return entropy(probabilities, base=2)\n","\n","def engineer_file_name_features(df, engineered_features):\n","    df['file_name_entropy'] = df['file.name'].fillna('').apply(calculate_entropy)\n","    engineered_features['file_name_entropy'] = df['file_name_entropy']\n","    return engineered_features\n","\n","# Main feature engineering function\n","def engineer_features(df):\n","    df_cleaned = clean_data(load_and_sample_data(df))\n","    engineered_features = pd.DataFrame()\n","\n","    # Apply all feature engineering steps\n","    engineered_features = engineer_event_action_features(df_cleaned, engineered_features)\n","    engineered_features = engineer_event_type_features(df_cleaned, engineered_features)\n","    engineered_features = engineer_process_executable_features(df_cleaned, engineered_features)\n","    engineered_features = engineer_process_name_features(df_cleaned, engineered_features)\n","    engineered_features = engineer_parent_process_features(df_cleaned, engineered_features)\n","    engineered_features = engineer_file_name_features(df_cleaned, engineered_features)\n","\n","    # Add 'target-class-name' and 'target-class' columns to the end of engineered_features DataFrame\n","    engineered_features['target-class'] = df_cleaned['target-class']\n","\n","    if \"target-class-name\" in df_cleaned.columns:\n","        engineered_features['target-class-name'] = df_cleaned['target-class-name']\n","    elif \"target\" in df_cleaned.columns:\n","        engineered_features['target'] = df_cleaned['target']      #the data drift csv has these other columns so had to put them here without breaking the code using the if condition. Unmet condition reverts back to default.\n","\n","    #the data drift csv has these other columns so had to put them here without breaking the code using the if condition. Unmet condition reverts back to default.\n","    if 'phase'in df_cleaned.columns:\n","        engineered_features['phase'] = df_cleaned['phase']\n","\n","    return engineered_features\n","\n","# Save each DataFrame with engineered features to a new CSV file in each subfolder\n","def process_and_save_features(root_folder):\n","    dataframes = read_csvs_from_subfolders(root_folder)\n","    for i, (df, subfolder) in enumerate(zip(dataframes, os.listdir(root_folder))):\n","        subfolder_path = os.path.join(root_folder, subfolder)\n","        engineered_features = engineer_features(df)\n","\n","        # Create \"Engineered_Features\" directory if it doesn't exist\n","        output_dir = os.path.join(subfolder_path, \"Engineered_Features\")\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Save engineered features to CSV\n","        output_path = os.path.join(output_dir, \"processed.csv\")\n","        engineered_features.to_csv(output_path, index=False)\n","        print(f\"Processed {subfolder} and saved engineered features to {output_path}\")\n","\n","#Engineer all features here:\n","root_folder = '/content/drive/MyDrive/Colab Notebooks/paper'\n","process_and_save_features(root_folder)\n"],"metadata":{"id":"2yDS3v5VVNa2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732285138102,"user_tz":-660,"elapsed":194304,"user":{"displayName":"Jamil","userId":"02624495780327096902"}},"outputId":"65a54dc2-43aa-4ec0-f99c-fa5d3f8175da"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed drift-data and saved engineered features to /content/drive/MyDrive/Colab Notebooks/paper/drift-data/Engineered_Features/processed.csv\n"]}]}]}